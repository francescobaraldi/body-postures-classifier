\section{Discussione dei modelli}\label{sec:modelli}
Il problema in esame è un problema di classificazione perché è necessario appunto "classificare" la postura di un soggetto tra cinque possibili posizioni. Gli algoritmi di machine learning adatti a questo tipo di problema sono diversi, i principali sono la \textit{softmax regression},  è come la logisitc regression ma per il caso multiclasse e utilizza la funzione softmax invece della funzione logisitica; il \textit{classificatore naive Bayes}, basato principalmente sul teorema della probabilità di Bayes; il \textit{K-Nearest neighbors}, che assume che sample simili siano vicini nello spazio delle features, quindi calcola la distanza relativa tra le features e per ogni sample prende i K samples più vicini e ne restituisce la moda delle label. Il parametro K è quindi un iperparametro da scegliere opportunamente. Il \textbf{decision tree}, o \textbf{alber decisionale} in italiano, è un metodo di apprendimento nel quale i dati sono suddivisi in base a un certo parametro in sottoinsiemi sempre più piccoli, fino ad arrivare ad avere insiemi che non sono più divisibili, questi saranno le foglie dell'albero e definiranno la label. In particolare gli alberi decisionali sono composti da:
\begin{itemize}
\item \textbf{nodi}: controllano il valore di un determinato parametro sui dati,
\item \textbf{rami}: collegano più nodi e rappresentano l'esito di un test fatto dal nodo padre,
\item \textbf{foglie}: sono i nodi terminali e determinano il risultato finale.
\end{itemize}

Per selezionare le features su cui effettuare il test e il valore \textit{cutpoint}, cioè il valore limite da testare, si utilizza il \textbf{recursive binary splitting} che ad ogni passo seleziona le features e il cutpoint che riducono maggiormente l'impurità dei nodi, un nodo è puro quando tutti i samples appartenenti alla medesima suddivisione hanno la stessa label.  L'indice per misurare l'impurità è un iperparametro e può essere scelto, una possibile soluzione è usare l'indice di Gini, cioè la probabilità che un una variabile sia classificata in modo errato. Uno dei vantaggi degli alberi decisionali è che sono semplici da costruire e veloci, inoltre escludono in modo automatico le features non importanti, ha però lo svantaggio di tendere facilmente all'overfitting. 

Un altro modello adatto a questo problema è il \textbf{support vector machine} (SVM), infatti il SVM ha lo scopo di dividere i sample in classi con un iperpiano di uno spazio n-dimensionale, dove n è il numero delle features. L'iperpiano da scegliere è quello con il massimo margine, cioè la massima distanza tra sample di classi diverse. Un aspetto fondamentale del SVM è che se i sample non fossero divisibili in modo lineare è possibile applicare una trasfomazione non lineare ai dati, per portarli in uno spazio con più dimensioni nel quale sono divisibili in modo lineare, la funzione che effettua questa trasformazione non lineare è detta \textbf{kernel} e può essere scelta in base al problema specifico, possibili scelte sono il kernel polinomiale o la radial basis function (RBF). Inoltre non sempre i sample sono divisibili perfettamente ma è necessario commettere degli errori, in questo caso si utilizza il \textbf{coefficiente di regolarizzazione} (C), un iperparametro per gestire il bilanciamento tra errori commessi e la grandezza del margine: per C piccolo si dà più importanza al margine, mentre per C grande si cerca di evitare più errori possibile. L'SVM ha il vantaggio di occupare poca memoria ed essere molto efficace in caso di dimensionalità delle features molto elevata, grazie all'utilizzo dei kernel.

L'ultimo metodo di learning approfondito sono le \textbf{reti neurali}. Una rete è composta da diversi \textit{layer}, e ogni layer ha diversi \textit{nodi}. Le features sono messe in input al primo layer, che a sua volta calcolerà una combinazione lineare degli ingressi e poi ne farà una trasformazione non lineare secondo una certa \textit{funzione di attivazione}, dopodiché l'output del primo layer è mandato in input al secondo layer. Questo processo è iterato fino ad arrivare all'ultimo layer, che calcolerà l'output finale. Il numero di layer e il numero di nodi per ogni layer è un iperparametro da scegliere opportunamente, inoltre è possibile scegliere la funzione di attivazione da applicare ai dati, in particolare le funzioni cosiddette "S-shaped", come la funzione sigmoide e la funzione tangente iperbolica (tanh) caratterizzano le reti chiamate \textit{multilayer perceptron} (MLP).  Un'altra componente fondamentale delle reti neurali sono i pesi che ogni collegamento tra nodi di livelli diversi ha, questi pesi sono i valori che il modello deve imparare durante la fase di training, per farlo le reti neurali usano il \textbf{error backporpagation}: si confronta il valore in uscita con il valore reale atteso, e in base all'errore, cioè alla differenza tra i due, si modificano i pesi. Un alto numero di layer e di nodi può portare a overfitting e ad alta complessità, per evitare questo problema di utilizzano delle tecniche di regolarizzazione, come il \textbf{weight decay} e l'\textbf{early stopping}. Un difetto delle reti neurali è la loro lentezza nel training dovuta proprio all'applicazione dell'algoritmo di error backpropagation, infatti le reti neurali richiedono molte risorse computazionali e l'addestramento risulta quindi lento.

Per questo progetto è stato scelto di utilizzare tre dei modelli descritti:
\begin{itemize}
\item Decision Tree,
\item Support Vector Machine
\item Reti Neurali
\end{itemize}

In particolare per ogni modello saranno studiate tre versioni:
\begin{itemize}
\item versione 1: i modelli sono addestrati con i dati grezzi e originali, senza pre-processing,
\item versione 2: i modelli sono addestrati con i dati pre-processati,
\item versione 3: si utilizza una versione ensemble della versione migliore tra le prime due
\end{itemize}